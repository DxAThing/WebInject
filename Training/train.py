# ============================================================
# train.py â€” é«˜å¯ç”¨ U-Net ICC æ˜ å°„ç½‘ç»œè®­ç»ƒå™¨
# ============================================================
#
# ã€æŠ¢å å¼å®ä¾‹ (Spot Instance) éƒ¨ç½²è¯´æ˜ã€‘
#
# ç”±äºè®­ç»ƒç¯å¢ƒä¸ºæŠ¢å å¼å®ä¾‹ï¼Œå®ä¾‹å¯èƒ½åœ¨ä»»æ„æ—¶åˆ»è¢«å›æ”¶ã€‚
# æœ¬è„šæœ¬å·²å†…ç½®å®Œæ•´çš„æ–­ç‚¹ç»­ä¼  (Auto-Resume) æœºåˆ¶:
#   - å¯åŠ¨æ—¶è‡ªåŠ¨æ£€æµ‹å¹¶æ¢å¤æœ€æ–° Checkpoint
#   - ä¿å­˜æ—¶ä½¿ç”¨åŸå­æ“ä½œ (å…ˆå†™ .tmp å† rename)ï¼Œé˜²æ­¢å†™å…¥ä¸­æ–­å¯¼è‡´æŸå
#
# æ¨èåœ¨å®ä¾‹å¯åŠ¨è„šæœ¬ä¸­ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼Œå®ç°"è¢«æŠ¢å åè‡ªåŠ¨é‡å¯":
#
#   #!/bin/bash
#   cd /path/to/Training
#   while true; do
#       python train.py
#       echo "è®­ç»ƒè¿›ç¨‹é€€å‡º (exit code: $?)ï¼Œ5 ç§’åé‡å¯..."
#       sleep 5
#   done
#
# æˆ–ä½¿ç”¨ systemd é…ç½®è‡ªåŠ¨é‡å¯:
#
#   [Unit]
#   Description=WebInject U-Net Training
#   After=network.target
#
#   [Service]
#   Type=simple
#   WorkingDirectory=/path/to/Training
#   ExecStart=/usr/bin/python train.py
#   Restart=always
#   RestartSec=10
#
#   [Install]
#   WantedBy=multi-user.target
#
# ============================================================

import os
import time
import datetime

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader

from config import TRAIN_CONFIG, MONITORS
from model import UNet
from dataset import LMDBDataset


# ======================= è¾…åŠ©å‡½æ•° ==========================


def get_device() -> torch.device:
    """è‡ªåŠ¨é€‰æ‹©æœ€ä½³è®¡ç®—è®¾å¤‡ã€‚"""
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"[Device] CUDA: {torch.cuda.get_device_name(0)}")
    elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        device = torch.device("mps")
        print("[Device] Apple MPS")
    else:
        device = torch.device("cpu")
        print("[Device] CPU")
    return device


def get_checkpoint_path(checkpoint_dir: str, monitor_name: str) -> str:
    """è·å–æŸä¸ª Monitor çš„æœ€æ–° Checkpoint è·¯å¾„ã€‚"""
    return os.path.join(checkpoint_dir, f"{monitor_name}_latest.pth")


def get_epoch_checkpoint_path(
    checkpoint_dir: str, monitor_name: str, epoch: int
) -> str:
    """è·å–æŸä¸ª Monitor æŸä¸ª Epoch çš„ Checkpoint è·¯å¾„ã€‚"""
    return os.path.join(checkpoint_dir, f"{monitor_name}_epoch_{epoch:04d}.pth")


def save_checkpoint(
    path: str,
    model: nn.Module,
    optimizer: optim.Optimizer,
    scheduler,
    epoch: int,
    loss: float,
) -> None:
    """
    åŸå­ä¿å­˜ Checkpointã€‚
    å…ˆå†™å…¥ .tmp æ–‡ä»¶ï¼Œå†é€šè¿‡ os.replace åŸå­è¦†ç›–ç›®æ ‡æ–‡ä»¶ï¼Œ
    é˜²æ­¢åœ¨ä¿å­˜ç¬é—´å®ä¾‹ä¸­æ–­å¯¼è‡´æƒé‡æ–‡ä»¶æŸåã€‚
    """
    state = {
        "epoch": epoch,
        "model_state_dict": model.state_dict(),
        "optimizer_state_dict": optimizer.state_dict(),
        "scheduler_state_dict": scheduler.state_dict() if scheduler else None,
        "loss": loss,
        "timestamp": datetime.datetime.now().isoformat(),
    }

    tmp_path = path + ".tmp"
    torch.save(state, tmp_path)
    os.replace(tmp_path, path)  # åŸå­æ“ä½œ


def load_checkpoint(
    path: str,
    model: nn.Module,
    optimizer: optim.Optimizer,
    scheduler,
    device: torch.device,
) -> int:
    """
    åŠ è½½ Checkpointï¼Œæ¢å¤æ‰€æœ‰è®­ç»ƒçŠ¶æ€ã€‚
    è¿”å›: ä¸Šæ¬¡å®Œæˆçš„ epoch ç¼–å· (ä¸‹æ¬¡è®­ç»ƒä» epoch+1 å¼€å§‹)ã€‚
    """
    checkpoint = torch.load(path, map_location=device, weights_only=False)

    model.load_state_dict(checkpoint["model_state_dict"])
    optimizer.load_state_dict(checkpoint["optimizer_state_dict"])

    if scheduler and checkpoint.get("scheduler_state_dict"):
        scheduler.load_state_dict(checkpoint["scheduler_state_dict"])

    epoch = checkpoint["epoch"]
    loss = checkpoint.get("loss", float("inf"))
    timestamp = checkpoint.get("timestamp", "unknown")

    print(f"  âœ“ ä» Checkpoint æ¢å¤: Epoch {epoch}, Loss {loss:.6f}, ä¿å­˜äº {timestamp}")

    return epoch


def get_lmdb_path(monitor_name: str) -> str:
    """è·å–æŸä¸ª Monitor çš„ LMDB è·¯å¾„ã€‚"""
    return os.path.join(TRAIN_CONFIG["LMDB_DIR"], f"{monitor_name}.lmdb")


# ======================= å•ä¸ª Monitor è®­ç»ƒ =================


def train_one_monitor(monitor_name: str, device: torch.device) -> None:
    """ä¸ºå•ä¸ª Monitor è®­ç»ƒä¸€ä¸ª U-Net ç½‘ç»œ N_dã€‚"""

    print("\n" + "=" * 60)
    print(f"è®­ç»ƒ Monitor: {monitor_name}")
    print("=" * 60)

    # --- è·¯å¾„å‡†å¤‡ ---
    checkpoint_dir = TRAIN_CONFIG["CHECKPOINT_DIR"]
    os.makedirs(checkpoint_dir, exist_ok=True)

    lmdb_path = get_lmdb_path(monitor_name)
    if not os.path.exists(lmdb_path):
        print(f"  [!] LMDB ä¸å­˜åœ¨: {lmdb_path}")
        print(f"      è¯·å…ˆè¿è¡Œ pack_data.py æ‰“åŒ…æ•°æ®ã€‚")
        return

    # --- Dataset & DataLoader ---
    dataset = LMDBDataset(
        lmdb_path=lmdb_path,
        crop_size=TRAIN_CONFIG["CROP_SIZE"],
        is_training=True,
    )

    if len(dataset) == 0:
        print(f"  [!] æ•°æ®é›†ä¸ºç©ºï¼Œè·³è¿‡ {monitor_name}")
        return

    dataloader = DataLoader(
        dataset,
        batch_size=TRAIN_CONFIG["BATCH_SIZE"],
        shuffle=True,
        num_workers=TRAIN_CONFIG["NUM_WORKERS"],
        pin_memory=TRAIN_CONFIG["PIN_MEMORY"],
        drop_last=True,
    )

    print(f"  æ ·æœ¬æ•°: {len(dataset)}")
    print(f"  Batch æ•°: {len(dataloader)}")

    # --- æ¨¡å‹ + ä¼˜åŒ–å™¨ + è°ƒåº¦å™¨ ---
    model = UNet(in_channels=3, out_channels=3).to(device)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=TRAIN_CONFIG["LEARNING_RATE"])
    scheduler = optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=TRAIN_CONFIG["NUM_EPOCHS"]
    )

    # --- æ–­ç‚¹ç»­ä¼ : Auto-Resume ---
    start_epoch = 0
    ckpt_path = get_checkpoint_path(checkpoint_dir, monitor_name)

    if os.path.isfile(ckpt_path):
        print(f"  å‘ç° Checkpoint: {ckpt_path}")
        start_epoch = load_checkpoint(ckpt_path, model, optimizer, scheduler, device)
        start_epoch += 1  # ä»ä¸‹ä¸€ä¸ª epoch å¼€å§‹

    if start_epoch >= TRAIN_CONFIG["NUM_EPOCHS"]:
        print(f"  âœ“ {monitor_name} å·²å®Œæˆå…¨éƒ¨ {TRAIN_CONFIG['NUM_EPOCHS']} ä¸ª Epochï¼Œè·³è¿‡ã€‚")
        return

    print(f"  ä» Epoch {start_epoch} å¼€å§‹è®­ç»ƒ (å…± {TRAIN_CONFIG['NUM_EPOCHS']} Epoch)")
    print(f"  LR: {TRAIN_CONFIG['LEARNING_RATE']}, Batch: {TRAIN_CONFIG['BATCH_SIZE']}")

    # --- è®­ç»ƒå¾ªç¯ ---
    for epoch in range(start_epoch, TRAIN_CONFIG["NUM_EPOCHS"]):
        model.train()
        epoch_loss = 0.0
        batch_count = 0
        epoch_start = time.time()

        for batch_idx, (inputs, targets) in enumerate(dataloader):
            inputs = inputs.to(device, non_blocking=True)
            targets = targets.to(device, non_blocking=True)

            # Forward
            outputs = model(inputs)
            loss = criterion(outputs, targets)

            # Backward
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()
            batch_count += 1

            # æ¯ 10 ä¸ª batch æ‰“å°ä¸€æ¬¡è¿›åº¦
            if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(dataloader):
                print(
                    f"  Epoch [{epoch + 1}/{TRAIN_CONFIG['NUM_EPOCHS']}] "
                    f"Batch [{batch_idx + 1}/{len(dataloader)}] "
                    f"Loss: {loss.item():.6f}",
                    end="\r",
                )

        # Epoch ç»“æŸ
        scheduler.step()
        avg_loss = epoch_loss / max(batch_count, 1)
        elapsed = time.time() - epoch_start
        current_lr = optimizer.param_groups[0]["lr"]

        print(
            f"  Epoch [{epoch + 1}/{TRAIN_CONFIG['NUM_EPOCHS']}] "
            f"Avg Loss: {avg_loss:.6f} | "
            f"LR: {current_lr:.6f} | "
            f"Time: {elapsed:.1f}s"
        )

        # --- ä¿å­˜ Checkpoint ---
        if (epoch + 1) % TRAIN_CONFIG["SAVE_INTERVAL"] == 0:
            # ä¿å­˜ latest (åŸå­æ“ä½œ)
            save_checkpoint(
                ckpt_path, model, optimizer, scheduler, epoch, avg_loss
            )

            # æ¯ 10 ä¸ª epoch é¢å¤–ä¿å­˜ä¸€ä»½å¸¦ epoch ç¼–å·çš„å‰¯æœ¬
            if (epoch + 1) % 10 == 0:
                epoch_path = get_epoch_checkpoint_path(
                    checkpoint_dir, monitor_name, epoch + 1
                )
                save_checkpoint(
                    epoch_path, model, optimizer, scheduler, epoch, avg_loss
                )
                print(f"  ğŸ’¾ Checkpoint å·²ä¿å­˜: {epoch_path}")

    print(f"\n  âœ“ {monitor_name} è®­ç»ƒå®Œæˆ!")


# ======================= ä¸»å…¥å£ ============================


def main():
    print("=" * 60)
    print("WebInject U-Net ICC æ˜ å°„ç½‘ç»œè®­ç»ƒå™¨")
    print(f"å¯åŠ¨æ—¶é—´: {datetime.datetime.now().isoformat()}")
    print("=" * 60)

    device = get_device()

    # è®ºæ–‡è¦æ±‚: ä¸ºæ¯ä¸ª Monitor è®­ç»ƒä¸€ä¸ªç½‘ç»œ N_d
    for monitor_name in MONITORS:
        train_one_monitor(monitor_name, device)

    print("\n" + "=" * 60)
    print("å…¨éƒ¨ Monitor è®­ç»ƒå®Œæˆ!")
    print("=" * 60)


if __name__ == "__main__":
    main()
